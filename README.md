# LLM Tracer ğŸš€

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109+-green.svg)](https://fastapi.tiangolo.com)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://docker.com)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)

A **LangSmith-like** observability platform for LangChain and LangGraph applications. Self-hosted, open-source, and easy to deploy.

## âœ¨ Features

- **ğŸ” Full LLM Observability** - Track all LLM calls, inputs, outputs, token usage, and latency
- **ğŸ› ï¸ Tool Call Tracking** - Monitor tool/function calls with inputs, outputs, and errors
- **ğŸ“Š Beautiful Dashboard** - React-based waterfall view of traces with detailed inspection
- **ğŸ”‘ API Key Management** - Secure authentication for your applications
- **ğŸ“ Project Organization** - Organize traces by project
- **ğŸš€ Auto-Instrumentation** - Just call `init()` and all LangChain calls are traced automatically (like Sentry!)
- **ğŸ’¾ Self-Hosted** - Full control over your data
- **ğŸ³ Docker Ready** - One command to deploy with Docker Compose
- **ğŸ˜ PostgreSQL + SQLite** - Production PostgreSQL with SQLite fallback for development

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Your Application                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚  LangChain  â”‚â—€â”€â”€â”€â”‚   llm-tracer-sdk    â”‚                     â”‚
â”‚  â”‚     LLM     â”‚    â”‚  (auto-instrument)  â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚ HTTP/API (async background)
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       LLM Tracer Server                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚   FastAPI   â”‚â”€â”€â”€â–¶â”‚   Piccolo   â”‚â”€â”€â”€â–¶â”‚  PostgreSQL â”‚         â”‚
â”‚  â”‚     API     â”‚    â”‚     ORM     â”‚    â”‚  / SQLite   â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                React Dashboard (Vite + Tailwind)            â”‚â”‚
â”‚  â”‚   â€¢ Traces List  â€¢ Trace Detail  â€¢ Projects/Keys           â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Project Structure

```
llm-tracer/
â”œâ”€â”€ packages/
â”‚   â”œâ”€â”€ llm-tracer/              # Server package (PyPI: llm-tracer)
â”‚   â”‚   â””â”€â”€ src/llm_tracer/
â”‚   â”‚       â”œâ”€â”€ api/v1/          # Versioned REST API
â”‚   â”‚       â”œâ”€â”€ db/models/       # Database models
â”‚   â”‚       â”œâ”€â”€ app.py           # FastAPI app factory
â”‚   â”‚       â””â”€â”€ config.py        # Settings management
â”‚   â””â”€â”€ llm-tracer-sdk/          # SDK package (PyPI: llm-tracer-sdk)
â”‚       â””â”€â”€ src/llm_tracer_sdk/
â”‚           â”œâ”€â”€ callback.py      # LangChain callback handler
â”‚           â”œâ”€â”€ instrumentation.py # Auto-patching
â”‚           â””â”€â”€ sdk.py           # Main interface
â”œâ”€â”€ apps/
â”‚   â””â”€â”€ web/                     # React dashboard (Vite + Tailwind)
â”œâ”€â”€ .github/workflows/           # CI/CD (Docker, PyPI, Tests)
â”œâ”€â”€ docker-compose.yml           # Production deployment
â”œâ”€â”€ docker-compose.dev.yml       # Development with SQLite
â”œâ”€â”€ Dockerfile                   # Multi-stage build
â””â”€â”€ pyproject.toml               # Workspace configuration
```

## ğŸš€ Quick Start

### Option 1: Docker (Recommended)

```bash
# Clone the repository
git clone https://github.com/yourusername/llm-tracer.git
cd llm-tracer

# Copy and configure environment
cp .env.example .env
# Edit .env to set SECRET_KEY

# Start with PostgreSQL
docker compose up -d

# Or use the development setup (SQLite)
docker compose -f docker-compose.dev.yml up -d
```

The server will be available at `http://localhost:8000`

### Option 2: Use Pre-built Docker Image

```bash
# Pull from GitHub Container Registry
docker pull ghcr.io/yourusername/llm-tracer:latest

# Run with SQLite
docker run -d -p 8000:8000 \
  -e DATABASE_URL=sqlite:///data/llmtracer.db \
  -e SECRET_KEY=your-secret-key \
  -v llmtracer_data:/app/data \
  ghcr.io/yourusername/llm-tracer:latest
```

### Option 3: Local Development

```bash
# Prerequisites: Python 3.10+, uv, Node.js 20+

# Install uv (if not already)
curl -LsSf https://astral.sh/uv/install.sh | sh  # macOS/Linux
# powershell -c "irm https://astral.sh/uv/install.ps1 | iex"  # Windows

# Clone and setup
git clone https://github.com/yourusername/llm-tracer.git
cd llm-tracer

# Install Python dependencies
uv sync

# Run the server
cd packages/llm-tracer
uv run llm-tracer

# In another terminal, run the frontend
cd apps/web
npm install
npm run dev
```

## ğŸ”Œ SDK Integration

### Installation

```bash
pip install llm-tracer-sdk
# or
uv add llm-tracer-sdk
```

### Basic Usage

```python
import llm_tracer_sdk

# Initialize once at startup
llm_tracer_sdk.init(
    api_key="lt-your-api-key",
    endpoint="http://localhost:8000"
)

# All LangChain calls are now automatically traced!
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(model="gpt-4")
response = llm.invoke("Hello!")  # Automatically traced!

# Add context to traces
with llm_tracer_sdk.set_user("user-123"):
    with llm_tracer_sdk.set_session("session-456"):
        response = llm.invoke("Who are you?")

# Shutdown gracefully (flushes pending traces)
llm_tracer_sdk.shutdown()
```

### Environment Variables

```bash
export LLM_TRACER_ENDPOINT=http://localhost:8000
export LLM_TRACER_API_KEY=lt-your-api-key
export LLM_TRACER_PROJECT=my-project
```

```python
# With env vars set, just call init()
llm_tracer_sdk.init()
```

### Manual Callback Handler

```python
from llm_tracer_sdk import LLMTracerCallback

# Use with LangChain's callback system
handler = LLMTracerCallback(
    api_key="lt-your-api-key",
    endpoint="http://localhost:8000"
)

llm = ChatOpenAI(model="gpt-4", callbacks=[handler])
response = llm.invoke("Hello!")
```

## ğŸ” Configuration

### Server Configuration

| Variable | Description | Default |
|----------|-------------|---------|
| `DATABASE_URL` | Database connection string | `sqlite:///llmtracer.db` |
| `SECRET_KEY` | Secret for signing tokens | Required in production |
| `HOST` | Server host | `0.0.0.0` |
| `PORT` | Server port | `8000` |
| `DEBUG` | Enable debug mode | `false` |
| `CORS_ORIGINS` | Comma-separated origins | `http://localhost:3000` |

### SDK Configuration

| Variable | Description | Default |
|----------|-------------|---------|
| `LLM_TRACER_ENDPOINT` | Server URL | Required |
| `LLM_TRACER_API_KEY` | API key | Required |
| `LLM_TRACER_PROJECT` | Project name | `default` |
| `LLM_TRACER_ENABLED` | Enable tracing | `true` |

## ğŸŒ API Endpoints

The server exposes a REST API under `/api/v1`:

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/v1/auth/login` | POST | User authentication |
| `/api/v1/auth/register` | POST | User registration |
| `/api/v1/projects` | GET/POST | List/create projects |
| `/api/v1/api-keys` | GET/POST | List/create API keys |
| `/api/v1/traces` | GET | List traces |
| `/api/v1/traces/{id}` | GET | Get trace details |
| `/api/v1/ingest` | POST | SDK trace ingestion |
| `/health` | GET | Health check |

## ğŸ§ª Development

```bash
# Install all dependencies
uv sync --all-extras

# Run tests
uv run pytest

# Run tests with coverage
uv run pytest --cov=packages

# Type checking
uv run mypy packages/

# Linting and formatting
uv run ruff check packages/
uv run ruff format packages/

# Frontend development
cd apps/web
npm install
npm run dev

# Build frontend
npm run build
```

## ğŸš¢ Deployment

### Docker Compose (Production)

```bash
# Configure environment
cp .env.example .env
# Set SECRET_KEY, POSTGRES_PASSWORD, etc.

# Deploy
docker compose up -d

# View logs
docker compose logs -f app
```

### GitHub Actions

The repository includes GitHub Actions workflows:

- **CI** (`ci.yml`): Runs tests, linting, and frontend build on every push
- **Docker** (`docker-publish.yml`): Builds and pushes Docker images to GHCR on release
- **PyPI** (`publish.yml`): Publishes packages to PyPI on release

### Manual Docker Build

```bash
# Build the image
docker build -t llm-tracer:latest .

# Run
docker run -d -p 8000:8000 \
  -e DATABASE_URL=sqlite:///data/llmtracer.db \
  -e SECRET_KEY=your-secret-key \
  llm-tracer:latest
```

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

## ğŸ¤ Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for contribution guidelines.
